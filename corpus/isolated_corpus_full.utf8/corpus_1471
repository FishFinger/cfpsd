From - Thu Dec 10 14:26:48 2009
X-Mozilla-Status: 0001
X-Mozilla-Status2: 00000000
Delivered-To: jpprost@gmail.com
Received: by 10.114.60.5 with SMTP id i5cs384133waa;
        Sat, 5 May 2007 16:37:28 -0700 (PDT)
Received: by 10.35.134.19 with SMTP id l19mr8174193pyn.1178408248332;
        Sat, 05 May 2007 16:37:28 -0700 (PDT)
Return-Path: <rasmusse@ptd.net>
Received: from vacuum.ics.mq.edu.au (vacuum.ics.mq.edu.au [137.111.216.16])
        by mx.google.com with ESMTP id f78si10740105pyh.2007.05.05.16.37.26;
        Sat, 05 May 2007 16:37:28 -0700 (PDT)
Received-SPF: fail (google.com: domain of rasmusse@ptd.net does not designate 137.111.216.16 as permitted sender)
Received: from vesuvius.ics.mq.edu.au (vesuvius.ics.mq.edu.au [137.111.240.11])
	by vacuum.ics.mq.edu.au (8.13.8+Sun/8.13.8) with ESMTP id l45NbPkQ000116
	for <jpprost@gmail.com>; Sun, 6 May 2007 09:37:25 +1000 (EST)
Received: from barra2.its.mq.edu.au (barra2.its.mq.edu.au [137.111.1.22])
	by vesuvius.ics.mq.edu.au (8.13.5/8.13.8) with ESMTP id l45NbPwT012850
	for <jpprost@mailbox.ics.mq.edu.au>; Sun, 6 May 2007 09:37:25 +1000 (EST)
X-ASG-Debug-ID: 1178408232-0e00001f0000-RCdhz5
X-Barracuda-URL: http://barra.its.mq.edu.au:8000/cgi-bin/mark.cgi
X-Barracuda-Connect: vacuum.ics.mq.edu.au[137.111.216.16]
X-Barracuda-Start-Time: 1178408232
Received: from vacuum.ics.mq.edu.au (vacuum.ics.mq.edu.au [137.111.216.16])
	by barra2.its.mq.edu.au (Spam Firewall) with ESMTP id EE84E7FC09E
	for <jpprost@mailbox.ics.mq.edu.au>; Sun,  6 May 2007 09:37:12 +1000 (EST)
Received: from baldrick.ocs.mq.edu.au (baldrick.ocs.mq.edu.au [137.111.1.12])
	by vacuum.ics.mq.edu.au (8.13.8+Sun/8.13.8) with ESMTP id l45NbCIX000087
	for <jpprost@ics.mq.edu.au>; Sun, 6 May 2007 09:37:12 +1000 (EST)
Received: from pm11.mailnet.ptd.net (pm11.mailnet.ptd.net [204.186.29.105])
	by baldrick.ocs.mq.edu.au (8.13.8/8.13.8) with ESMTP id l45Nb9os018443
	for <jpprost@ics.mq.edu.au>; Sun, 6 May 2007 09:37:10 +1000 (EST)
Received: (qmail 324 invoked by uid 50005); 5 May 2007 23:37:08 -0000
Received: from 70.44.168.179 by pm11.mailnet.ptd.net (envelope-from <rasmusse@ptd.net>, uid 50002) with qmail-scanner-2.01 
 (clamdscan: 0.90.2/3183.  
 Clear:RC:0(70.44.168.179):. 
 Processed in 0.572077 secs); 05 May 2007 23:37:08 -0000
Received: from unknown (HELO ARCSOffice) (authenticated:rasmusse@[70.44.168.179])
          (envelope-sender <rasmusse@ptd.net>)
          by pm11.mailnet.ptd.net (qmail-ldap-1.03) with SMTP
          for <rasmusse@ptd.net>; 5 May 2007 23:37:06 -0000
Message-ID: <00d901c78f6e$4943e7b0$b3a82c46@ARCSOffice>
Reply-To: "Priscilla Rasmussen" <rasmusse@ptd.net>
From: "Priscilla Rasmussen" <rasmusse@ptd.net>
To: <rasmusse@ptd.net>
X-ASG-Orig-Subj: First NLG Challenge on Attribute Selection for Referring Expressions Generation
Subject: First NLG Challenge on Attribute Selection for Referring Expressions Generation
Date: Sat, 5 May 2007 19:36:55 -0400
Organization: ACL/AMTA/ARCS/dg.o
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_000_00D6_01C78F4C.BCEF3BF0"
X-Priority: 3
X-MSMail-Priority: Normal
X-Mailer: Microsoft Outlook Express 6.00.2900.3028
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.3028
X-Barracuda-Virus-Scanned: by Barracuda Spam Firewall at its.mq.edu.au
X-Barracuda-Spam-Score: 0.00
X-Barracuda-Spam-Status: No, SCORE=0.00 using per-user scores of TAG_LEVEL=3.0 QUARANTINE_LEVEL=3.5 KILL_LEVEL=1000.0 tests=HTML_MESSAGE
X-Barracuda-Spam-Report: Code version 3.1, rules version 3.1.16158
	Rule breakdown below
	 pts rule name              description
	---- ---------------------- --------------------------------------------------
	0.00 HTML_MESSAGE           BODY: HTML included in message

This is a multi-part message in MIME format.

------=_NextPart_000_00D6_01C78F4C.BCEF3BF0
Content-Type: text/plain;
	charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

CALL FOR PARTICIPATION

First NLG Challenge on Attribute Selection for Referring Expressions
Generation


The field of Natural Language Generation (NLG) has strong evaluation
traditions, in particular in user-based evaluation of applied
systems.  However, while in most other NLP fields shared-task
evaluation now plays an important role, there are few results of this
kind in NLG.  The Shared Task Evaluation Campaign (STEC) in Generation
of Referring Expressions (GRE) is intended to be a first step in the
direction of exploring what is required for shared-task evaluation in
NLG.  Under the umbrella of this GRE STEC, we are planning to organise
a series of evaluation events, involving, over time, a wide range of
GRE task definitions, data resources and evaluation methods.

As a first step, and in order to gauge community interest, we are
setting up a pilot evaluation in the spirit of a feasibility test: the
Attribute Selection for Referring Expressions Generation Challenge.
This Challenge will be presented and discussed at this year's UCNLG+MT
Workshop in Copenhagen, on 11 September, at MT Summit XI.  If
successful, we plan to organise a larger-scale event in 2008,
extending the remit to cover aspects of GRE beyond attribute selection
as well as more data resources and evaluation methods.

With this call, we would like to invite researchers from all
backgrounds to participate in the Attribute Selection for Referring
Expressions Generation Challenge.  The focus will be on selecting
attributes for generation of distinguishing descriptions, and
submissions will be evaluated against a shared data set of
human-authored descriptions elicited in a visual domain (see below for
more details).


Background
----------

The GRE STEC initiative arose as a direct result of the NSF Workshop
on Shared Tasks and Comparative Evaluation in NLG held in Arlington,
US, in April 2007 (http://www.ling.ohio-state.edu/~mwhite/nlgeval07/).
The workshop provided a forum for discussion on the prospects, pros
and cons of STECs in NLG, and the related question of shared resources.

During the Arlington Workshop, several of the position papers made
reference to GRE as a prime candidate for a STEC, since this area has
been the focus of intensive research over the past decade, leading to
greater consensus over basic problem definition, inputs and outputs
than in most NLG subfields.  One of the break-out groups at the
workshop was given the task of working out how a GRE STEC could be
organised, and our plans described below represent the output of the
break-out group's work.

The report on the workshop, jointly authored by the participants and
due to be published later this year, reflects the variety of opinion
on the subject of shared-task evaluations in NLG, including concerns
that a restrictive selection of tasks and evaluation methods may
narrow a particular field's research focus and produce misleading
evaluation results, respectively.  Our plans for the GRE STEC address
these concerns through strategies for ensuring diversity in tasks and
evaluation methods as well as grass-roots community involvement.


The GRE STEC
------------

We conceive of the GRE STEC as one element of a possible constellation
of shared tasks in NLG, each focusing on different aspects of the
field.  We are committed to addressing a wide range of task
definitions (attribute selection, pronominalisation and other
anaphoric reference, realisation, etc.), different data resources
(COCONUT, TUNA, GREC, etc.), and different evaluation methods
(correlation measures, set overlap, surface similarity metrics, as
well as user-oriented evaluations).  Most importantly, we will
encourage grass-roots involvement through calls for the submission of
task proposals, data resources and evaluation methods.


The Attribute Selection for GRE Challenge at UCNLG+MT
-----------------------------------------------------

The Attribute Selection for GRE Challenge has the role of a pilot
challenge for the longer-term GRE STEC.  The choice of shared task is
motivated by the fact that much of the research in GRE has focussed on
the task of selecting attributes for intended referents in a knowledge
base, in such a way that all potential `distractors' are ruled out:

"Given a symbol corresponding to an intended referent, how do we
work out the semantic content of a referring expression that uniquely
identifies the entity in question?" (Bohnet and Dale, 2005, p. 1004)

The Shared Task:
- - - - - - - -

Data: The data is from the TUNA corpus of referring expressions.  This
choice is mainly motivated by the fact that the corpus was designed
specifically to address attribute selection in GRE.  Instances in the
corpus comprise (a) the kind of information required in the input to
attribute selection for GRE (referent type and ID, possible attributes
and potential distractors), and (b) output sets of attributes as
derived from human-authored descriptions for the intended referent.
More details on the TUNA corpus can be obtained from the URLs listed
at the end of this message.

Task: Submitted systems should implement the task of mapping a given
input representation to a (single) attribute set that identifies the
intended referent.  The aim here may be either to select any
distinguishing set of attributes, or the minimal number of attributes
that uniquely identifies the referent, or to select attribute sets as
humans would (outputs will be evaluated against both minimal sets and
human-produced sets).

Evaluation:  We will be using two default methods for evaluation:

  a. The Dice coefficient of similarity used to compare system outputs
  to (i) attribute sets derived from human-produced descriptions
  (these are of the same type as the outputs included in the training
  and development data); and (ii) minimal distinguishing attribute
  sets (the TUNA domain has unique minimal sets for each domain
  entity);

  b. A small-scale human-based experiment for evaluating submitted
  system-generated descriptions, in which subjects are given the task
  of identifying referents given system-generated descriptions.

We may also use additional evaluation methods submitted by
participants under the Evaluation Methods Track (see below).
Participants in the Shared-Task Track will be notified of any
additional evaluation methods that will be used, and may opt out of
these additional evaluations.

Submission Procedure and Tracks:
- - - - - - - - - - - - - - - -

We will release a Participants' Pack including a sample of the data
(paired inputs and outputs) to registered participants on 15
May, and thereafter to new participants on registration.  The
corresponding training and development sets will be available =
immediately
after the registration deadline, and the test set (inputs only) will
be made available for download three weeks before the deadline for the
final submission of results.  During the three weeks between release
of test data and final submission deadline, participants will have one
week from the time of download to generate and submit their
outputs.  Prior to downloading the test data, participants are required
to submit a description of their method and development set results
(the idea is that the method is not changed during the week available
for generating test data outputs).

We invite submissions in one or more of the following three tracks:

1. The shared task proper: Participants are asked to create automatic
methods for selecting attribute sets for given inputs as described
above.  Participation involves submitting a description of the method
and results for the development set just before downloading the test
data, and submitting (single) outputs for each input in the test data
set by the final results deadline (for provisional timeline see below),
taking no more than one week from the time of download.

2. Open submission category: Participants may also devise their own
task definition using the training and development data, and submit a
research paper reporting their method and results in this open
category.  In this first evaluation round we will not invite multiple
submissions for the same task under this track.  However, we may
include task definitions submitted under this tack as shared tasks in
future evaluation rounds.

3. Evaluation techniques: We also invite proposals for evaluation
methods to be used in the evaluation of attribute selection for GRE.
Here, we distinguish two subtracks:

  a. Research papers describing GRE evaluation methods and, optionally,
  results for development data: participants may devise any evaluation
  method for GRE, apply it to some data and submit a paper reporting
  the results under this subtrack.  We are planning to make available
  development data output sets from the Shared Task Track for use in
  this subtrack.

  b. Ready-to-use Perl scripts or executable java jar files:
  participants may submit evaluation scripts which we will use to
  evaluate the test set outputs, provided the scripts are fully
  documented and only use standard libraries (however, if there are
  problems executing a script we cannot guarantee that we will use
  it).  Scripts will need to operate on outputs and reference
  attribute sets (a precise specification of the output format will be
  distributed with the Participants Pack).


Proceedings and presentations
-----------------------------

Paper submissions under all three tracks will be included in the
proceedings of the UCNLG+MT Workshop which will be published by the MT
Summit XI organisers.  Papers will not undergo a selection procedure =
with
multiple reviews, but the organisers reserve the right to reject =
material
which is not appropriate given the participation guidelines.  Page =
limits
are the same for all tracks: papers should not exceed three (3) pages in
length, including diagrams and bibliography.

Participants who are able to attend the UCNLG+MT Workshop will be =
invited
to give a short presentation based on their paper.


Participation
-------------

At this point we would like anybody who is potentially interested in
participating in the Attribute Selection for GRE Challenge to send us
an email at the address below in order to register.  We will
distribute a Participants' Pack on 15 May, which will give full
details of the Challenge, including input and output specifications
for shared task and evaluation methods.


Provisional timeline
--------------------

1-31 May        Registration open
15 May          Release of Participants' Packs
1 June          Release of training and development data
7-28 July       Test data download and submission of test data outputs;
                This is a 3-step process:
                  1. Submission of 3-page papers describing approach and
                     development set results
                  2. Test data made available for download
                  3. Test data results due 1 week after download, but
                     no later than 28 July
28 July         Final submission deadline for test set outputs
11 September    Attribute Selection for GRE Session at UCNLG+MT


Organisers
----------

Anja Belz, Brighton University, UK
Albert Gatt, Aberdeen University, UK
Ehud Reiter, Aberdeen University, UK
Jette Viethen, Macquarie University, Australia


Contact email address
---------------------

gre-stec (at) itri.brighton.ac.uk


Websites
--------

Attribute Selection for GRE Challenge: =
http://www.csd.abdn.ac.uk/research/evaluation/

UCNLG+MT: http://www.itri.brighton.ac.uk/ucnlg

TUNA Corpus: http://www.csd.abdn.ac.uk/research/tuna/corpus/

------=_NextPart_000_00D6_01C78F4C.BCEF3BF0
Content-Type: text/html;
	charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD>
<META http-equiv=3DContent-Type content=3D"text/html; =
charset=3Diso-8859-1">
<META content=3D"MSHTML 6.00.2900.3059" name=3DGENERATOR>
<STYLE></STYLE>
</HEAD>
<BODY bgColor=3D#ffffff>
<DIV>
<P class=3DMsoNormal style=3D"MARGIN: 0in 0in 0pt">CALL FOR=20
PARTICIPATION<BR><BR>First NLG Challenge on Attribute Selection for =
Referring=20
Expressions<BR>Generation<BR><BR><BR>The field of Natural Language =
Generation=20
(NLG) has strong evaluation<BR>traditions, in particular in user-based=20
evaluation of applied<BR>systems.&nbsp; However, while in most other NLP =
fields=20
shared-task<BR>evaluation now plays an important role, there are few =
results of=20
this<BR>kind in NLG.&nbsp; The Shared Task Evaluation Campaign (STEC) in =

Generation<BR>of Referring Expressions (GRE) is intended to be a first =
step in=20
the<BR>direction of exploring what is required for shared-task =
evaluation=20
in<BR>NLG.&nbsp; Under the umbrella of this GRE STEC, we are planning to =

organise<BR>a series of evaluation events, involving, over time, a wide =
range=20
of<BR>GRE task definitions, data resources and evaluation =
methods.<BR><BR>As a=20
first step, and in order to gauge community interest, we are<BR>setting =
up a=20
pilot evaluation in the spirit of a feasibility test: the<BR>Attribute =
Selection=20
for Referring Expressions Generation Challenge.<BR>This Challenge will =
be=20
presented and discussed at this year's UCNLG+MT<BR>Workshop in =
<?xml:namespace=20
prefix =3D st1 ns =3D "urn:schemas-microsoft-com:office:smarttags" =
/><st1:City=20
w:st=3D"on"><st1:place w:st=3D"on">Copenhagen</st1:place></st1:City>, on =
11=20
September, at MT Summit XI.&nbsp; If<BR>successful, we plan to organise =
a=20
larger-scale event in 2008,<BR>extending the remit to cover aspects of =
GRE=20
beyond attribute selection<BR>as well as more data resources and =
evaluation=20
methods.<BR><BR>With this call, we would like to invite researchers from =

all<BR>backgrounds to participate in the Attribute Selection for=20
Referring<BR>Expressions Generation Challenge.&nbsp; The focus will be =
on=20
selecting<BR>attributes for generation of distinguishing descriptions,=20
and<BR>submissions will be evaluated against a shared data set=20
of<BR>human-authored descriptions elicited in a visual domain (see below =

for<BR>more details).<BR><BR><BR>Background<BR>----------<BR><BR>The GRE =
STEC=20
initiative arose as a direct result of the NSF Workshop<BR>on Shared =
Tasks and=20
Comparative Evaluation in NLG held in <st1:place w:st=3D"on"><st1:City=20
w:st=3D"on">Arlington</st1:City>,<BR><st1:country-region=20
w:st=3D"on">US</st1:country-region></st1:place>, in April 2007 (<A=20
href=3D"http://www.ling.ohio-state.edu/~mwhite/nlgeval07/">http://www.lin=
g.ohio-state.edu/~mwhite/nlgeval07/</A>).<BR>The=20
workshop provided a forum for discussion on the prospects, pros<BR>and =
cons of=20
STECs in NLG, and the related question of shared =
resources.<BR><BR>During the=20
Arlington Workshop, several of the position papers made<BR>reference to =
GRE as a=20
prime candidate for a STEC, since this area has<BR>been the focus of =
intensive=20
research over the past decade, leading to<BR>greater consensus over =
basic=20
problem definition, inputs and outputs<BR>than in most NLG =
subfields.&nbsp; One=20
of the break-out groups at the<BR>workshop was given the task of working =
out how=20
a GRE STEC could be<BR>organised, and our plans described below =
represent the=20
output of the<BR>break-out group's work.<BR><BR>The report on the =
workshop,=20
jointly authored by the participants and<BR>due to be published later =
this year,=20
reflects the variety of opinion<BR>on the subject of shared-task =
evaluations in=20
NLG, including concerns<BR>that a restrictive selection of tasks and =
evaluation=20
methods may<BR>narrow a particular field's research focus and produce=20
misleading<BR>evaluation results, respectively.&nbsp; Our plans for the =
GRE STEC=20
address<BR>these concerns through strategies for ensuring diversity in =
tasks=20
and<BR>evaluation methods as well as grass-roots community=20
involvement.<BR><BR><BR>The GRE STEC<BR>------------<BR><BR>We conceive =
of the=20
GRE STEC as one element of a possible constellation<BR>of shared tasks =
in NLG,=20
each focusing on different aspects of the<BR>field.&nbsp; We are =
committed to=20
addressing a wide range of task<BR>definitions (attribute selection,=20
pronominalisation and other<BR>anaphoric reference, realisation, etc.),=20
different data resources<BR>(COCONUT, TUNA, GREC, etc.), and different=20
evaluation methods<BR>(correlation measures, set overlap, surface =
similarity=20
metrics, as<BR>well as user-oriented evaluations).&nbsp; Most =
importantly, we=20
will<BR>encourage grass-roots involvement through calls for the =
submission=20
of<BR>task proposals, data resources and evaluation =
methods.<BR><BR><BR>The=20
Attribute Selection for GRE Challenge at=20
UCNLG+MT<BR>-----------------------------------------------------<BR><BR>=
The=20
Attribute Selection for GRE Challenge has the role of a =
pilot<BR>challenge for=20
the longer-term GRE STEC.&nbsp; The choice of shared task =
is<BR>motivated by the=20
fact that much of the research in GRE has focussed on<BR>the task of =
selecting=20
attributes for intended referents in a knowledge<BR>base, in such a way =
that all=20
potential `distractors' are ruled out:<BR><BR>"Given a symbol =
corresponding to=20
an intended referent, how do we<BR>work out the semantic content of a =
referring=20
expression that uniquely<BR>identifies the entity in question?" (Bohnet =
and=20
Dale, 2005, p. 1004)<BR><BR>The Shared Task:<BR>- - - - - - - =
-<BR><BR>Data: The=20
data is from the TUNA corpus of referring expressions.&nbsp; =
This<BR>choice is=20
mainly motivated by the fact that the corpus was =
designed<BR>specifically to=20
address attribute selection in GRE.&nbsp; Instances in the<BR>corpus =
comprise=20
(a) the kind of information required in the input to<BR>attribute =
selection for=20
GRE (referent type and ID, possible attributes<BR>and potential =
distractors),=20
and (b) output sets of attributes as<BR>derived from human-authored =
descriptions=20
for the intended referent.<BR>More details on the TUNA corpus can be =
obtained=20
from the URLs listed<BR>at the end of this message.<BR><BR>Task: =
Submitted=20
systems should implement the task of mapping a given<BR>input =
representation to=20
a (single) attribute set that identifies the<BR>intended referent.&nbsp; =
The aim=20
here may be either to select any<BR>distinguishing set of attributes, or =
the=20
minimal number of attributes<BR>that uniquely identifies the referent, =
or to=20
select attribute sets as<BR>humans would (outputs will be evaluated =
against both=20
minimal sets and<BR>human-produced sets).<BR><BR>Evaluation:&nbsp; We =
will be=20
using two default methods for evaluation:<BR><BR>&nbsp; a. The Dice =
coefficient=20
of similarity used to compare system outputs<BR>&nbsp; to (i) attribute =
sets=20
derived from human-produced descriptions<BR>&nbsp; (these are of the =
same type=20
as the outputs included in the training<BR>&nbsp; and development data); =
and=20
(ii) minimal distinguishing attribute<BR>&nbsp; sets (the TUNA domain =
has unique=20
minimal sets for each domain<BR>&nbsp; entity);<BR><BR>&nbsp; b. A =
small-scale=20
human-based experiment for evaluating submitted<BR>&nbsp; =
system-generated=20
descriptions, in which subjects are given the task<BR>&nbsp; of =
identifying=20
referents given system-generated descriptions.<BR><BR>We may also use =
additional=20
evaluation methods submitted by<BR>participants under the Evaluation =
Methods=20
Track (see below).<BR>Participants in the Shared-Task Track will be =
notified of=20
any<BR>additional evaluation methods that will be used, and may opt out=20
of<BR>these additional evaluations.<BR><BR>Submission Procedure and =
Tracks:<BR>-=20
- - - - - - - - - - - - - - -<BR><BR>We will release a Participants' =
Pack=20
including a sample of the data<BR>(paired inputs and outputs) to =
registered=20
participants on 15<BR>May, and thereafter to new participants on=20
registration.&nbsp; The<BR>corresponding training and development sets =
will be=20
available immediately<BR>after the registration deadline, and the test =
set=20
(inputs only) will<BR>be made available for download three weeks before =
the=20
deadline for the<BR>final submission of results.&nbsp; During the three =
weeks=20
between release<BR>of test data and final submission deadline, =
participants will=20
have one<BR>week from the time of download to generate and submit=20
their<BR>outputs.&nbsp; Prior to downloading the test data, participants =
are=20
required<BR>to submit a description of their method and development set=20
results<BR>(the idea is that the method is not changed during the week=20
available<BR>for generating test data outputs).<BR><BR>We invite =
submissions in=20
one or more of the following three tracks:<BR><BR>1. The shared task =
proper:=20
Participants are asked to create automatic<BR>methods for selecting =
attribute=20
sets for given inputs as described<BR>above.&nbsp; Participation =
involves=20
submitting a description of the method<BR>and results for the =
development set=20
just before downloading the test<BR>data, and submitting (single) =
outputs for=20
each input in the test data<BR>set by the final results deadline (for=20
provisional timeline see below),<BR>taking no more than one week from =
the time=20
of download.<BR><BR>2. Open submission category: Participants may also =
devise=20
their own<BR>task definition using the training and development data, =
and submit=20
a<BR>research paper reporting their method and results in this=20
open<BR>category.&nbsp; In this first evaluation round we will not =
invite=20
multiple<BR>submissions for the same task under this track.&nbsp; =
However, we=20
may<BR>include task definitions submitted under this tack as shared =
tasks=20
in<BR>future evaluation rounds.<BR><BR>3. Evaluation techniques: We also =
invite=20
proposals for evaluation<BR>methods to be used in the evaluation of =
attribute=20
selection for GRE.<BR>Here, we distinguish two subtracks:<BR><BR>&nbsp; =
a.=20
Research papers describing GRE evaluation methods and, =
optionally,<BR>&nbsp;=20
results for development data: participants may devise any =
evaluation<BR>&nbsp;=20
method for GRE, apply it to some data and submit a paper =
reporting<BR>&nbsp; the=20
results under this subtrack.&nbsp; We are planning to make =
available<BR>&nbsp;=20
development data output sets from the Shared Task Track for use =
in<BR>&nbsp;=20
this subtrack.<BR><BR>&nbsp; b. Ready-to-use Perl scripts or executable =
java jar=20
files:<BR>&nbsp; participants may submit evaluation scripts which we =
will use=20
to<BR>&nbsp; evaluate the test set outputs, provided the scripts are=20
fully<BR>&nbsp; documented and only use standard libraries (however, if =
there=20
are<BR>&nbsp; problems executing a script we cannot guarantee that we =
will=20
use<BR>&nbsp; it).&nbsp; Scripts will need to operate on outputs and=20
reference<BR>&nbsp; attribute sets (a precise specification of the =
output format=20
will be<BR>&nbsp; distributed with the Participants=20
Pack).<BR><BR><BR>Proceedings and=20
presentations<BR>-----------------------------<BR><BR>Paper submissions =
under=20
all three tracks will be included in the<BR>proceedings of the UCNLG+MT =
Workshop=20
which will be published by the MT<BR>Summit XI organisers.&nbsp; Papers =
will not=20
undergo a selection procedure with<BR>multiple reviews, but the =
organisers=20
reserve the right to reject material<BR>which is not appropriate given =
the=20
participation guidelines.&nbsp; Page limits<BR>are the same for all =
tracks:=20
papers should not exceed three (3) pages in<BR>length, including =
diagrams and=20
bibliography.<BR><BR>Participants who are able to attend the UCNLG+MT =
Workshop=20
will be invited<BR>to give a short presentation based on their=20
paper.<BR><BR><BR>Participation<BR>-------------<BR><BR>At this point we =
would=20
like anybody who is potentially interested in<BR>participating in the =
Attribute=20
Selection for GRE Challenge to send us<BR>an email at the address below =
in order=20
to register.&nbsp; We will<BR>distribute a Participants' Pack on 15 May, =
which=20
will give full<BR>details of the Challenge, including input and output=20
specifications<BR>for shared task and evaluation =
methods.<BR><BR><BR>Provisional=20
timeline<BR>--------------------<BR><BR>1-31=20
May&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Registration open<BR>15=20
May&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Release of=20
Participants' Packs<BR>1=20
June&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Release of =
training=20
and development data<BR>7-28 July&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =
Test data=20
download and submission of test data=20
outputs;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=
nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
This is a 3-step=20
process:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=
nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
1. Submission of 3-page papers describing approach=20
and<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
development set=20
results<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n=
bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
2. Test data made available for=20
download<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=
nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
3. Test data results due 1 week after download,=20
but<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
no later than 28 July<BR>28 =
July&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
Final submission deadline for test set outputs<BR>11 =
September&nbsp;&nbsp;&nbsp;=20
Attribute Selection for GRE Session at=20
UCNLG+MT<BR><BR><BR>Organisers<BR>----------<BR><BR>Anja Belz, Brighton=20
University, UK<BR>Albert Gatt, Aberdeen University, UK<BR>Ehud Reiter, =
Aberdeen=20
University, UK<BR>Jette Viethen, Macquarie University,=20
Australia<BR><BR><BR>Contact email=20
address<BR>---------------------<BR><BR>gre-stec (at)=20
itri.brighton.ac.uk<BR><BR><BR>Websites<BR>--------<BR><BR>Attribute =
Selection=20
for GRE Challenge: <A=20
href=3D"http://www.csd.abdn.ac.uk/research/evaluation/">http://www.csd.ab=
dn.ac.uk/research/evaluation/</A><BR><BR>UCNLG+MT:=20
<A=20
href=3D"http://www.itri.brighton.ac.uk/ucnlg">http://www.itri.brighton.ac=
.uk/ucnlg</A><BR><BR>TUNA=20
Corpus: <A=20
href=3D"http://www.csd.abdn.ac.uk/research/tuna/corpus/">http://www.csd.a=
bdn.ac.uk/research/tuna/corpus/</A></P></DIV></BODY></HTML>

------=_NextPart_000_00D6_01C78F4C.BCEF3BF0--

