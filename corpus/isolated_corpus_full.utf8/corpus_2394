From - Thu Dec 10 14:29:12 2009
X-Mozilla-Status: 0001
X-Mozilla-Status2: 00000000
Delivered-To: jpprost@gmail.com
Received: by 10.114.60.10 with SMTP id i10cs228905waa;
        Tue, 8 Jan 2008 02:56:22 -0800 (PST)
Received: by 10.66.251.20 with SMTP id y20mr875129ugh.67.1199789777971;
        Tue, 08 Jan 2008 02:56:17 -0800 (PST)
Return-Path: <corpora-bounces@uib.no>
Received: from noralf.uib.no (noralf.uib.no [129.177.30.12])
        by mx.google.com with ESMTP id x23si629137ugc.58.2008.01.08.02.55.44;
        Tue, 08 Jan 2008 02:56:17 -0800 (PST)
Received-SPF: pass (google.com: domain of corpora-bounces@uib.no designates 129.177.30.12 as permitted sender) client-ip=129.177.30.12;
Authentication-Results: mx.google.com; spf=pass (google.com: domain of corpora-bounces@uib.no designates 129.177.30.12 as permitted sender) smtp.mail=corpora-bounces@uib.no
Received: from localhost (noralf.uib.no) [127.0.0.1] 
	by noralf.uib.no  with esmtp  (Exim 4.34)
	id 1JCC4j-0004Rd-MQ; Tue, 08 Jan 2008 11:52:53 +0100
Received: from rolf.uib.no [129.177.30.19] 
	by noralf.uib.no for corpora@lists.uib.no with esmtp  (Exim 4.34)
	id 1JCC4c-0004RL-R8; Tue, 08 Jan 2008 11:52:46 +0100
Received: from smarthost146.mail.easynet.fr (smtp4.mail.easynet.fr)
	[212.180.1.146] 
	by rolf.uib.no for corpora@uib.no with esmtp  (Exim 4.34)
	id 1JCC4W-0005eH-Sw; Tue, 08 Jan 2008 11:52:46 +0100
Received: from bob75-1-81-57-229-125.fbx.proxad.net ([81.57.229.125]
	helo=[192.168.1.236])
	by smtp4.mail.easynet.fr with esmtpa (Exim 4.63)
	(envelope-from <info@elda.org>)
	id 1JCC4C-0000ls-1j; Tue, 08 Jan 2008 11:52:20 +0100
Message-ID: <478355E5.4050500@elda.org>
Date: Tue, 08 Jan 2008 11:52:21 +0100
From: ELDA <info@elda.org>
User-Agent: Thunderbird 2.0.0.9 (Windows/20071031)
MIME-Version: 1.0
To: destinataires inconnus:;
X-checked-clean: by exiscan on rolf
X-Scanner: 5ec5e2bd0c6e1966076ad48dfedbe91b http://tjinfo.uib.no/virus.html
X-UiB-SpamFlag: NO UIB: 0 hits, 8.0 required
X-UiB-SpamReport: spamassassin found;
 nothing
Subject: [Corpora-List] ELRA Workshop on Evaluation at LREC 2008 - 1st Call
	for Papers
X-BeenThere: corpora@uib.no
X-Mailman-Version: 2.1.9
Precedence: list
List-Id: <corpora.uib.no>
List-Unsubscribe: <http://mailman.uib.no/listinfo/corpora>,
	<mailto:corpora-request@uib.no?subject=unsubscribe>
List-Archive: <http://www.uib.no/mailman/public/corpora>
List-Post: <mailto:corpora@uib.no>
List-Help: <mailto:corpora-request@uib.no?subject=help>
List-Subscribe: <http://mailman.uib.no/listinfo/corpora>,
	<mailto:corpora-request@uib.no?subject=subscribe>
Content-Type: text/plain; charset="windows-1252"
Content-Transfer-Encoding: quoted-printable
Sender: corpora-bounces@uib.no
Errors-To: corpora-bounces@uib.no

[Apologies for multiple postings]

CALL FOR PAPERS
ELRA Workshop on Evaluation
Looking into the Future of Evaluation: when automatic metrics meet =

task-based and performance-based approaches


To be held in conjunction with the 6th International Language Resources =

and Evaluation Conference (LREC 2008)
27 May 2008, Palais des Congr=E8s Mansour Eddahbi, Marrakech


Background

Automatic methods to evaluate system performance play an important role =

in the development of a language technology system. They speed up =

research and development by allowing fast feedback, and the idea is also =

to make results comparable while aiming to match human evaluation in =

terms of output evaluation. However, after several years of study and =

exploitation of such metrics we still face problems like the following ones:

* they only evaluate part of what should be evaluated
* they produce measurements that are hard to understand/explain, and/or =

hard to relate to the concept of quality
* they fail to match human evaluation
* they require resources that are expensive to create

etc. Therefore, an effort to integrate knowledge from a multitude of =

evaluation activities and methodologies should help us solve some of =

these immediate problems and avoid creating new metrics that reproduce =

such problems.


Looking at MT as a sample case, problems to be immediately pointed out =

are twofold: reference translations and distance measurement. The former =

are difficult and expensive to produce, they do not cover the usually =

wide spectrum of translation possibilities and what is even more =

discouraging, worse results are obtained when reference translations are =

of higher quality (more spontaneous and natural, and thus, sometimes =

more lexically and syntactically distant from the source text). =

Regarding the latter, the measurement of the distance between the source =

text and the output text is carried out by means of automatic metrics =

that do not match human intuition as well as claimed. Furthermore, =

different metrics perform differently, which has already led researchers =

to study metric/approach combinations which integrate automatic methods =

into a deeper linguistically oriented evaluation. Hopefully, this should =

help soften the unfair treatment received by some rule-based systems, =

clearly punished by certain system-approach sensitive metrics.


On the other hand, there is the key issue of =AB what needs to be measured =

=BB, so as to draw the conclusion that =AB something is of good quality =BB=
, =

or probably rather =AB something is useful for a particular purpose =BB. In =

this regard, works like those done within the FEMTI framework have shown =

that aspects such as usability, reliability, efficiency, portability, =

etc. should also be considered. However, the measuring of such quality =

characteristics cannot always be automated, and there may be many other =

aspects that could be usefully measured.


This workshop follows the evolution of a series of workshops where =

methodological problems, not only for MT but for evaluation in general, =

have been approached. Along the lines of these discussions and aiming to =

go one step further, the current workshop, while taking into account the =

advantages of automatic methods and the shortcomings of current methods, =

should focus on task-based and performance-based approaches for =

evaluation of natural language applications, with key questions such as:


- How can it be determined how useful a given system is for a given task?
- How can focusing on such issues and combining these approaches with =

our already acquired experience on automatic evaluation help us develop =

new metrics and methodologies which do not feature the shortcomings of =

current automatic metrics?
- Should we work on hybrid methodologies of automatic and human =

evaluation for certain technologies and not for others?
- Can we already envisage the integration of these approaches?
- Can we already plan for some immediate collaborations/experiments?
- What would it mean for the FEMTI framework to be extended to other HLT =

applications, such as summarization, IE, or QA? Which new aspects would =

it need to cover?


We solicit papers that address these questions and other related issues =

relevant to the workshop.


Workshop Programme and Audience Addressed

This full-day workshop is intended for researchers and developers on =

different evaluation technologies, with experience on the various issues =

concerned in the call, and interested in defining a methodology to move =

forward.

The workshop feature invited talks, submitted papers, and will conclude =

with a discussion on future developments and collaboration.


Workshop Chairing Team
Gregor Thurmair (Linguatec Sprachtechnologien GmbH, Germany) - chair
Khalid Choukri (ELDA - Evaluations and Language resources Distribution =

Agency, France) =96 co-chair
Bente Maegaard (CST, University of Copenhagen, Denmark) =96 co-chair


Organising Committee
Victoria Arranz (ELDA - Evaluations and Language resources Distribution =

Agency, France)
Khalid Choukri (ELDA - Evaluations and Language resources Distribution =

Agency, France)
Christopher Cieri (LDC - Linguistic Data Consortium, USA)
Eduard Hovy (Information Sciences Institute of the University of =

Southern California, USA)
Bente Maegaard (CST, University of Copenhagen, Denmark)
Keith J. Miller (The MITRE Corporation, USA)
Satoshi Nakamura (National Institute of Information and Communications =

Technology, Japan)
Andrei Popescu-Belis (IDIAP Research Institute, Switzerland)
Gregor Thurmair (Linguatec Sprachtechnologien GmbH, Germany)


Important dates

Deadline for abstracts: Monday 28 January 2008
Notification to Authors: Monday 3 March 2008
Submission of Final Version: Tuesday 25 March 2008
Workshop: Tuesday 27 May 2008


Submission Format
Abstracts should be no longer than 1500 words and should be submitted in =

PDF format to Gregor Thurmair at g.thurmair@linguatec.de.


_______________________________________________
Corpora mailing list
Corpora@uib.no
http://mailman.uib.no/listinfo/corpora
